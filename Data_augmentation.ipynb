{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mggMCsFyQXH",
        "outputId": "d498514f-82c9-442d-c056-628b8b25a653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n",
        "from albumentations import HorizontalFlip, VerticalFlip, Rotate\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.utils import CustomObjectScope\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Recall, Precision"
      ],
      "metadata": {
        "id": "jHQghk1_hVkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN0jHzO1yY-W",
        "outputId": "8c725b5c-d1d4-493b-a630-cadae7b6385b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8990\n",
            "Train:  7192\n",
            "Valid:  1798\n"
          ]
        }
      ],
      "source": [
        "def create_dir(path):\n",
        "    \"\"\" Create a directory. \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def load_data(path, split=0.2):\n",
        "    \"\"\" Load the images and masks \"\"\"\n",
        "    images = sorted(glob(f\"{path}/train images/*.png\"))\n",
        "    masks = sorted(glob(f\"{path}/train masks/*.png\"))\n",
        "    print(len(images))\n",
        "    \"\"\" Split the data \"\"\"\n",
        "    split_size = int(len(images) * split)\n",
        "    train_x, valid_x = train_test_split(images, test_size=split_size, random_state=42)\n",
        "    train_y, valid_y = train_test_split(masks, test_size=split_size, random_state=42)\n",
        "\n",
        "    return (train_x, train_y), (valid_x, valid_y)\n",
        "\n",
        "def augment_data(images, masks, save_path, augment=True):\n",
        "    \"\"\" Performing data augmentation. \"\"\"\n",
        "    H = 128\n",
        "    W = 128\n",
        "\n",
        "    for idx, (x, y) in tqdm(enumerate(zip(images, masks)), total=len(images)):\n",
        "        \"\"\" Extracting the dir name and image name \"\"\"\n",
        "        dir_name = x.split(\"/\")[-3]\n",
        "        name = dir_name + \"_\" + x.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "        \"\"\" Read the image and mask \"\"\"\n",
        "        x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "        y = cv2.imread(y, cv2.IMREAD_COLOR)\n",
        "\n",
        "        if augment == True:\n",
        "            aug = HorizontalFlip(p=1.0)\n",
        "            augmented = aug(image=x, mask=y)\n",
        "            x1 = augmented[\"image\"]\n",
        "            y1 = augmented[\"mask\"]\n",
        "\n",
        "            aug = VerticalFlip(p=1)\n",
        "            augmented = aug(image=x, mask=y)\n",
        "            x2 = augmented['image']\n",
        "            y2 = augmented['mask']\n",
        "\n",
        "            aug = Rotate(limit=45, p=1.0)\n",
        "            augmented = aug(image=x, mask=y)\n",
        "            x3 = augmented[\"image\"]\n",
        "            y3 = augmented[\"mask\"]\n",
        "\n",
        "            X = [x, x1, x2, x3]\n",
        "            Y = [y, y1, y2, y3]\n",
        "\n",
        "        else:\n",
        "            X = [x]\n",
        "            Y = [y]\n",
        "\n",
        "        idx = 0\n",
        "        for i, m in zip(X, Y):\n",
        "            i = cv2.resize(i, (W, H))\n",
        "            m = cv2.resize(m, (W, H))\n",
        "            m = m/255.0\n",
        "            m = (m > 0.5) * 255\n",
        "\n",
        "            if len(X) == 1:\n",
        "                tmp_image_name = f\"{name}.jpg\"\n",
        "                tmp_mask_name  = f\"{name}.jpg\"\n",
        "            else:\n",
        "                tmp_image_name = f\"{name}_{idx}.jpg\"\n",
        "                tmp_mask_name  = f\"{name}_{idx}.jpg\"\n",
        "\n",
        "            image_path = os.path.join(save_path, \"image/\", tmp_image_name)\n",
        "            mask_path  = os.path.join(save_path, \"mask/\", tmp_mask_name)\n",
        "\n",
        "            cv2.imwrite(image_path, i)\n",
        "            cv2.imwrite(mask_path, m)\n",
        "\n",
        "            idx += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" Load the dataset \"\"\"\n",
        "dataset_path = '/content/drive/MyDrive/dataset'\n",
        "(train_x, train_y), (valid_x, valid_y) = load_data(dataset_path, split=0.2)\n",
        "\n",
        "print(\"Train: \", len(train_x))\n",
        "print(\"Valid: \", len(valid_x))\n",
        "\n",
        "create_dir(\"new_data/train/image/\")\n",
        "create_dir(\"new_data/train/mask/\")\n",
        "create_dir(\"new_data/valid/image/\")\n",
        "create_dir(\"new_data/valid/mask/\")\n",
        "\n",
        "#augment_data(train_x, train_y, \"new_data/train/\", augment=True)\n",
        "#augment_data(valid_x, valid_y, \"new_data/valid/\", augment=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNWagMoV5iB0",
        "outputId": "f395155d-a1f6-4ad8-ddac-8d5602858f10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7192/7192 [33:08<00:00,  3.62it/s]\n",
            "100%|██████████| 1798/1798 [07:46<00:00,  3.86it/s]\n"
          ]
        }
      ],
      "source": [
        "augment_data(train_x, train_y, \"new_data/train/\", augment=True)\n",
        "augment_data(valid_x, valid_y, \"new_data/valid/\", augment=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Data augmentation - Image Segmentation of Spores.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNk6IXrnb7LHf+c8H/P+zvU"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}